<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Azure Speech Avatar with OpenAI Realtime API</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #0a1428 0%, #1a2744 50%, #2a3754 100%);
            color: #ffffff;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }

        .container {
            text-align: center;
            max-width: 1200px;
            width: 100%;
        }

        .header {
            margin-bottom: 40px;
        }

        .logo {
            background: linear-gradient(135deg, #007AFF 0%, #0056CC 100%);
            color: white;
            padding: 16px 32px;
            border-radius: 16px;
            font-weight: 800;
            font-size: 32px;
            letter-spacing: 2px;
            margin-bottom: 20px;
            display: inline-block;
            box-shadow: 0 8px 32px rgba(0, 122, 255, 0.4);
        }

        .title {
            font-size: 28px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .subtitle {
            color: #FF6B35;
            font-weight: 600;
            font-size: 18px;
            margin-bottom: 10px;
        }

        .system-status {
            background: rgba(0, 255, 100, 0.1);
            border: 2px solid rgba(0, 255, 100, 0.3);
            border-radius: 12px;
            padding: 15px;
            margin-bottom: 15px;
            text-align: left;
        }

        .system-status h3 {
            color: #00FF64;
            margin-bottom: 10px;
            font-size: 16px;
        }

        .system-status ul {
            font-size: 14px;
            color: #B0C4DE;
            line-height: 1.6;
            list-style: none;
            padding-left: 0;
        }

        .system-status li {
            margin-bottom: 5px;
            padding-left: 20px;
            position: relative;
        }

        .system-status li:before {
            content: "â€¢";
            color: #00FF64;
            position: absolute;
            left: 0;
        }

        .description {
            color: #B0C4DE;
            font-size: 16px;
            max-width: 700px;
            margin: 0 auto;
            line-height: 1.6;
        }

        .main-interface {
            display: flex;
            gap: 40px;
            align-items: flex-start;
            justify-content: center;
            flex-wrap: wrap;
            margin: 40px 0;
        }

        .avatar-section {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        /* Enhanced avatar container for Speech Avatar */
        .avatar-container {
            width: 640px;
            height: 360px;
            border-radius: 24px;
            background: #000;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 20px;
            box-shadow: 0 0 40px rgba(0, 122, 255, 0.3);
            transition: all 0.3s ease;
            border: 3px solid rgba(255, 255, 255, 0.1);
            position: relative;
            overflow: hidden;
        }

        .avatar-container.listening {
            box-shadow: 0 0 60px rgba(0, 255, 100, 0.6);
            border-color: rgba(0, 255, 100, 0.5);
        }

        .avatar-container.thinking {
            box-shadow: 0 0 60px rgba(255, 193, 7, 0.6);
            border-color: rgba(255, 193, 7, 0.5);
        }

        .avatar-container.speaking {
            box-shadow: 0 0 60px rgba(255, 107, 53, 0.8);
            border-color: rgba(255, 107, 53, 0.5);
        }

        #speechAvatarVideo {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        #avatarCanvas {
            width: 100%;
            height: 100%;
            display: none;
        }

        .avatar-placeholder {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 60px;
            color: rgba(255, 255, 255, 0.3);
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .loading-spinner {
            border: 3px solid rgba(255, 255, 255, 0.1);
            border-top: 3px solid #00ff88;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin-bottom: 20px;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .control-section {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
        }

        .control-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            font-size: 48px;
            background: linear-gradient(135deg, #00FF64 0%, #00CC51 100%);
            color: white;
            transition: all 0.15s ease;
            box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);
            position: relative;
            overflow: hidden;
        }

        .control-button:hover {
            transform: scale(1.05);
            box-shadow: 0 16px 48px rgba(0, 0, 0, 0.4);
        }

        .control-button:active {
            transform: scale(0.95);
        }

        .control-button.active {
            background: linear-gradient(135deg, #FF3B30 0%, #CC2E24 100%);
            animation: button-active 1s infinite;
        }

        @keyframes button-active {
            0%, 100% { box-shadow: 0 12px 40px rgba(255, 59, 48, 0.4); }
            50% { box-shadow: 0 16px 48px rgba(255, 59, 48, 0.6); }
        }

        .status {
            margin: 20px 0;
            font-weight: 600;
            color: #00FF64;
            font-size: 20px;
            min-height: 30px;
        }

        .metrics-panel {
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 12px;
            padding: 15px;
            margin: 15px 0;
            font-size: 14px;
            color: #B0C4DE;
            text-align: left;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 10px;
            margin-top: 10px;
        }

        .metric-item {
            background: rgba(0, 0, 0, 0.3);
            padding: 8px;
            border-radius: 8px;
        }

        .metric-label {
            color: #8B95A7;
            font-size: 12px;
            margin-bottom: 4px;
        }

        .metric-value {
            color: #FFFFFF;
            font-weight: 600;
            font-size: 16px;
        }

        /* Avatar configuration panel */
        .avatar-config {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 12px;
            padding: 15px;
            margin: 15px 0;
        }

        .config-group {
            margin: 10px 0;
        }

        .config-label {
            display: block;
            color: #8B95A7;
            font-size: 12px;
            margin-bottom: 5px;
        }

        .config-select {
            width: 100%;
            padding: 8px;
            border-radius: 8px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            background: rgba(255, 255, 255, 0.1);
            color: white;
            font-size: 14px;
        }

        .conversation-section {
            max-width: 900px;
            margin: 30px auto;
        }

        .conversation-header {
            font-size: 20px;
            font-weight: 600;
            margin-bottom: 15px;
            color: #B0C4DE;
        }

        .conversation {
            max-height: 400px;
            overflow-y: auto;
            text-align: left;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 20px;
            padding: 20px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .message {
            margin: 15px 0;
            padding: 15px 20px;
            border-radius: 20px;
            max-width: 85%;
            word-wrap: break-word;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .message.user {
            background: linear-gradient(135deg, rgba(0, 122, 255, 0.3) 0%, rgba(0, 122, 255, 0.1) 100%);
            margin-left: auto;
            margin-right: 0;
            border-bottom-right-radius: 5px;
        }

        .message.assistant {
            background: linear-gradient(135deg, rgba(255, 107, 53, 0.3) 0%, rgba(255, 107, 53, 0.1) 100%);
            margin-right: auto;
            margin-left: 0;
            border-bottom-left-radius: 5px;
        }

        .message.system {
            background: linear-gradient(135deg, rgba(255, 193, 7, 0.3) 0%, rgba(255, 193, 7, 0.1) 100%);
            margin: 0 auto;
            border-radius: 15px;
            font-size: 14px;
            max-width: 90%;
        }

        .message strong {
            color: #FFD700;
            display: block;
            margin-bottom: 5px;
            font-size: 14px;
            font-weight: 700;
        }

        .tech-info {
            margin-top: 40px;
            padding: 25px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 15px;
            font-size: 14px;
            color: #B0C4DE;
            backdrop-filter: blur(10px);
        }

        .tech-badges {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 15px;
        }

        .tech-badge {
            background: rgba(0, 122, 255, 0.2);
            color: #87CEEB;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: 600;
        }

        .tech-badge.active {
            background: rgba(0, 255, 100, 0.2);
            color: #90EE90;
        }

        .error-message {
            color: #FF6B6B;
            background: rgba(255, 107, 107, 0.1);
            padding: 15px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #FF6B6B;
        }

        .controls-group {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-top: 20px;
        }

        .secondary-button {
            padding: 8px 16px;
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            color: white;
            border-radius: 8px;
            cursor: pointer;
            font-size: 12px;
            transition: all 0.2s ease;
        }

        .secondary-button:hover {
            background: rgba(255, 255, 255, 0.2);
            border-color: rgba(255, 255, 255, 0.3);
        }

        @media (max-width: 768px) {
            .avatar-container {
                width: 100%;
                max-width: 480px;
                height: 270px;
            }
            
            .control-button {
                width: 100px;
                height: 100px;
                font-size: 40px;
            }
        }
    </style>
</head>
<body>
    <input type="hidden" id="clientId" value="{{ client_id }}">

    <div class="container">
        <div class="header">
            <div class="logo">AZURE HYBRID SYSTEM</div>
            <div class="title">Speech Services Avatar + OpenAI Realtime Audio</div>
            <div class="subtitle">Visual Avatar with Real-time Voice Processing</div>
            <div class="system-status">
                <h3>System Architecture</h3>
                <ul>
                    <li>Visual Avatar: Azure Speech Services (Avatar SDK)</li>
                    <li>Voice Processing: Azure OpenAI Realtime API</li>
                    <li>Speech Recognition: Continuous STT via Realtime API</li>
                    <li>Voice Synthesis: Avatar TTS with lip-sync</li>
                    <li>Knowledge Base: YPF minipywo integration</li>
                </ul>
            </div>
            <div class="description">
                Hybrid implementation combining Azure Speech Avatar for visual representation
                with Azure OpenAI Realtime API for advanced conversational capabilities.
            </div>
        </div>

        <div class="main-interface">
            <div class="avatar-section">
                <!-- Speech Avatar Container -->
                <div class="avatar-container" id="avatarContainer">
                    <video id="speechAvatarVideo" autoplay playsinline></video>
                    <canvas id="avatarCanvas"></canvas>
                    <div class="avatar-placeholder" id="avatarPlaceholder">
                        <div class="loading-spinner" id="loadingSpinner" style="display: none;"></div>
                        <span id="placeholderText">Avatar Offline</span>
                    </div>
                </div>
                
                <div class="status" id="status">System ready - Click to start</div>
                
                <div class="metrics-panel">
                    <div class="metrics-grid">
                        <div class="metric-item">
                            <div class="metric-label">OpenAI Realtime</div>
                            <div class="metric-value" id="realtimeStatus">Idle</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">Speech Avatar</div>
                            <div class="metric-value" id="avatarStatus">Disconnected</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">VAD Status</div>
                            <div class="metric-value" id="vadStatus">Inactive</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">Audio Quality</div>
                            <div class="metric-value" id="audioQuality">--</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">Processing</div>
                            <div class="metric-value" id="processingStatus">Ready</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">Session Time</div>
                            <div class="metric-value" id="sessionTime">00:00</div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="control-section">
                <button class="control-button" id="controlButton" onclick="toggleSession()">
                    <span id="controlButtonIcon">MIC</span>
                </button>
                <div style="color: #B0C4DE; font-size: 14px; max-width: 220px; text-align: center;">
                    Press to start hybrid voice session
                </div>
                
                <!-- Avatar Configuration -->
                <div class="avatar-config">
                    <div class="config-group">
                        <label class="config-label">Avatar Character</label>
                        <select class="config-select" id="avatarCharacter">
                            <option value="lisa">Lisa (Female)</option>
                            <option value="jason">Jason (Male)</option>
                            <option value="anna">Anna (Female)</option>
                            <option value="brian">Brian (Male)</option>
                        </select>
                    </div>
                    <div class="config-group">
                        <label class="config-label">Avatar Style</label>
                        <select class="config-select" id="avatarStyle">
                            <option value="casual-sitting">Casual Sitting</option>
                            <option value="graceful-sitting">Graceful Sitting</option>
                            <option value="technical-sitting">Technical Sitting</option>
                        </select>
                    </div>
                    <div class="config-group">
                        <label class="config-label">TTS Voice</label>
                        <select class="config-select" id="ttsVoice">
                            <option value="es-AR-TomasNeural">Tomas (AR Spanish)</option>
                            <option value="es-AR-ElenaNeural">Elena (AR Spanish)</option>
                            <option value="en-US-JennyNeural">Jenny (US English)</option>
                            <option value="en-US-GuyNeural">Guy (US English)</option>
                        </select>
                    </div>
                </div>
                
                <div class="controls-group">
                    <button class="secondary-button" onclick="clearConversation()">Clear Chat</button>
                    <button class="secondary-button" onclick="testAvatarSpeak()">Test Avatar</button>
                </div>
            </div>
        </div>

        <div class="conversation-section">
            <div class="conversation-header">Conversation Log</div>
            <div class="conversation" id="conversation">
                <div class="message system">
                    <strong>System:</strong>
                    Hybrid system ready. Azure Speech Avatar for visuals, OpenAI Realtime for processing.
                </div>
            </div>
        </div>

        <div class="tech-info">
            <strong>Technical Implementation:</strong><br>
            Hybrid architecture using Azure Speech Services for avatar visualization with lip-sync,
            combined with Azure OpenAI Realtime API for advanced conversational AI and voice processing.
            <div class="tech-badges">
                <span class="tech-badge active">Azure Speech Avatar</span>
                <span class="tech-badge active">OpenAI Realtime API</span>
                <span class="tech-badge active">WebRTC Video</span>
                <span class="tech-badge active">24kHz Audio</span>
                <span class="tech-badge active">Server VAD</span>
                <span class="tech-badge">YPF Integration</span>
            </div>
        </div>
    </div>

    <!-- Azure Cognitive Services Speech SDK -->
    <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
    
    <script>
        // ================================
        // GLOBAL CONFIGURATION
        // ================================
        
        // System state
        let sessionActive = false;
        let sessionStartTime = null;
        let sessionTimer = null;
        
        // Azure OpenAI Realtime WebSocket
        let realtimeWebSocket = null;
        let realtimeConfig = null;
        
        // Azure Speech Services
        let speechConfig = null;
        let speechSynthesizer = null;
        let avatarSynthesizer = null;
        let speechRecognizer = null;
        
        // WebRTC for Avatar
        let peerConnection = null;
        let avatarConnected = false;
        
        // Audio processing
        let audioContext = null;
        let mediaStream = null;
        let audioProcessor = null;
        let currentTranscription = "";
        
        // Response queue for avatar
        let responseQueue = [];
        let isAvatarSpeaking = false;
        
        // ================================
        // INITIALIZATION
        // ================================
        
        async function initializeSystem() {
            try {
                console.log('Initializing hybrid system...');
                updateStatus('Initializing system...');
                
                // Fetch configurations from backend
                const [realtimeResponse, speechResponse] = await Promise.all([
                    fetch('/api/voice-live-config'),
                    fetch('/api/speech-config')
                ]);
                
                if (!realtimeResponse.ok || !speechResponse.ok) {
                    throw new Error('Failed to fetch configuration');
                }
                
                realtimeConfig = await realtimeResponse.json();
                const speechConfigData = await speechResponse.json();
                
                // Initialize Speech SDK
                initializeSpeechSDK(speechConfigData);
                
                console.log('System initialized successfully');
                updateStatus('System ready - Click to start');
                return true;
                
            } catch (error) {
                console.error('Initialization failed:', error);
                showError(`Initialization error: ${error.message}`);
                return false;
            }
        }
        
        function initializeSpeechSDK(config) {
            // Create Speech configuration
            speechConfig = SpeechSDK.SpeechConfig.fromSubscription(
                config.speechKey,
                config.speechRegion
            );
            
            // Set language and voice
            speechConfig.speechRecognitionLanguage = 'es-AR';
            speechConfig.speechSynthesisVoiceName = document.getElementById('ttsVoice').value;
            
            console.log('Speech SDK initialized');
        }
        
        // ================================
        // AZURE OPENAI REALTIME CONNECTION
        // ================================
        
        async function connectToRealtimeAPI() {
            try {
                updateStatus('Connecting to OpenAI Realtime...');
                const endpoint = 'wss://ai-se45600a0809ai272711732013.openai.azure.com';
                const deployment = 'gpt-4o-realtime-preview';
                const apiKey = '7TZA2Og761nyAROkbOxzdkEGEkJaa60lZQTwwV2roxxDJGxcOw0DJQQJ99BGACHYHv6XJ3w3AAAAACOGE4Cj';
                const wsUrl = `${endpoint}/openai/deployments/${deployment}/realtime?api-version=2024-08-01-preview&api-key=${apiKey}`;
                console.log('Connecting to:', wsUrl);
                realtimeWebSocket = new WebSocket(wsUrl);
                realtimeWebSocket.onopen = () => {
                    console.log('Realtime WebSocket connected');
                    updateRealtimeStatus('Connected');
                    setupRealtimeSession();
                };
                realtimeWebSocket.onmessage = handleRealtimeMessage;
                realtimeWebSocket.onerror = (error) => {
                    console.error('Realtime WebSocket error:', error);
                    updateRealtimeStatus('Error');
                };
                realtimeWebSocket.onclose = () => {
                    console.log('Realtime WebSocket closed');
                    updateRealtimeStatus('Disconnected');
                };
            } catch (error) {
                console.error('Failed to connect to Realtime API:', error);
                throw error;
            }
        }
        
        function setupRealtimeSession() {
            const sessionConfig = {
                type: "session.update",
                session: {
                    instructions: "You are a helpful AI assistant. Provide clear and concise responses.",
                    voice: "alloy",
                    turn_detection: {
                        type: "server_vad",
                        threshold: 0.5,
                        prefix_padding_ms: 300,
                        silence_duration_ms: 500
                    },
                    input_audio_format: "pcm16",
                    output_audio_format: "pcm16",
                    input_audio_transcription: {
                        model: "whisper-1"
                    },
                    modalities: ["text", "audio"],
                    temperature: 0.7,
                    max_response_output_tokens: 4096,
                    // Removed unsupported parameters
                    tools: realtimeConfig.tools || [],
                    tool_choice: "auto"
                }
            };
            
            realtimeWebSocket.send(JSON.stringify(sessionConfig));
            console.log('Realtime session configured');
        }
        
        // async function handleRealtimeMessage(event) {
        //     try {
        //         const data = JSON.parse(event.data);
                
        //         switch (data.type) {
        //             case "session.created":
        //                 console.log('Realtime session created');
        //                 break;
                        
        //             case "session.updated":
        //                 console.log('Realtime session updated');
        //                 if (!sessionActive) {
        //                     await startAudioCapture();
        //                     sessionActive = true;
        //                     startSessionTimer();
        //                 }
        //                 break;
                        
        //             case "input_audio_buffer.speech_started":
        //                 console.log('Speech detected');
        //                 updateVADStatus('Active');
        //                 updateUI('listening');
        //                 break;
                        
        //             case "input_audio_buffer.speech_stopped":
        //                 console.log('Speech ended');
        //                 updateVADStatus('Inactive');
        //                 break;
                        
        //             case "conversation.item.input_audio_transcription.completed":
        //                 if (data.transcript) {
        //                     console.log('User said:', data.transcript);
        //                     addMessage('user', data.transcript);
        //                 }
        //                 break;
                        
        //             case "response.audio_transcript.done":
        //                 if (data.transcript) {
        //                     console.log('Assistant response:', data.transcript);
        //                     addMessage('assistant', data.transcript);
        //                     // Queue response for avatar to speak
        //                     queueAvatarResponse(data.transcript);
        //                 }
        //                 break;
                        
        //             case "response.done":
        //                 updateProcessingStatus('Ready');
        //                 break;
                        
        //             case "error":
        //                 console.error('Realtime API error:', data.error);
        //                 showError(`API Error: ${data.error?.message || 'Unknown error'}`);
        //                 break;
        //         }
                
        //     } catch (error) {
        //         console.error('Error handling Realtime message:', error);
        //     }
        // }

        async function handleRealtimeMessage(event) {
            try {
                const data = JSON.parse(event.data);
                switch (data.type) {
                    case "session.created":
                        console.log('Realtime session created');
                        break;
                    case "session.updated":
                        console.log('Realtime session updated');
                        if (!sessionActive) {
                            await startAudioCapture();
                            sessionActive = true;
                            startSessionTimer();
                        }
                        break;
                    case "conversation.item.input_audio_transcription.completed":
                        if (data.transcript) {
                            console.log('User said:', data.transcript);
                            addMessage('user', data.transcript);
                        }
                        break;
                    case "response.audio_transcript.done":
                        if (data.transcript) {
                            console.log('Assistant response:', data.transcript);
                            addMessage('assistant', data.transcript);
                            queueAvatarResponse(data.transcript);
                        }
                        break;
                    case "response.done":
                        updateProcessingStatus('Ready');
                        break;
                    case "error":
                        console.error('Realtime API error:', data.error);
                        if (data.error?.message) {
                            showError(`API Error: ${data.error.message}`);
                        }
                        break;
                    default:
                        // Log unhandled events for debugging
                        if (!data.type.includes('delta')) {
                            console.log('Unhandled event:', data.type);
                        }
                        break;
                }
            } catch (error) {
                console.error('Error handling Realtime message:', error);
            }
        }
        
        // ================================
        // AZURE SPEECH AVATAR
        // ================================
        
        // 
        
        async function initializeAvatar() {
            try {
                console.log('Initializing Speech Synthesizer...');
                updateAvatarStatus('Initializing...');

                // Only creaate TTS without WebRTC
                const audioConfig = SpeechSDK.AudioConfig.fromDefaultSpeakerOutput();
                avatarSynthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, audioConfig);

                updateAvatarStatus('TTS Ready');
                avatarConnected = true;
                document.getElementById('avatarPlaceholder').style.display = 'none';

                return true;

            } catch (error) {
                console.error('TTS initialization failed:', error);
                updateAvatarStatus('Failed');
                return false;
            }
        }
        
        // async function setupAvatarWebRTC() {
        //     const configuration = {
        //         iceServers: [
        //             { urls: 'stun:stun.l.google.com:19302' },
        //             { urls: 'stun:stun1.l.google.com:19302' }
        //         ]
        //     };
            
        //     peerConnection = new RTCPeerConnection(configuration);
            
        //     // Add transceivers
        //     peerConnection.addTransceiver('audio', { direction: 'recvonly' });
        //     peerConnection.addTransceiver('video', { direction: 'recvonly' });
            
        //     // Handle incoming streams
        //     peerConnection.ontrack = (event) => {
        //         console.log('Received track:', event.track.kind);
        //         if (event.track.kind === 'video') {
        //             const videoElement = document.getElementById('speechAvatarVideo');
        //             videoElement.srcObject = event.streams[0];
        //         }
        //     };
            
        //     // Create offer
        //     const offer = await peerConnection.createOffer();
        //     await peerConnection.setLocalDescription(offer);
            
        //     console.log('WebRTC setup complete');
        // }

        async function setupAvatarWebRTC() {
            // NOT use WebRTC with Speech SDK
            const audioConfig = SpeechSDK.AudioConfig.fromDefaultSpeakerOutput();

            // Create synthesizer
            avatarSynthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, audioConfig);

            console.log('Speech synthesizer created');
            return true;
        }
        
        // Queue and process avatar responses
        function queueAvatarResponse(text) {
            responseQueue.push(text);
            processAvatarQueue();
        }
        
        async function processAvatarQueue() {
            if (isAvatarSpeaking || responseQueue.length === 0) return;
            
            isAvatarSpeaking = true;
            const text = responseQueue.shift();
            
            updateUI('speaking');
            updateProcessingStatus('Avatar Speaking');
            
            try {
                if (avatarSynthesizer && avatarConnected) {
                    // Speak with avatar
                    await new Promise((resolve, reject) => {
                        avatarSynthesizer.speakTextAsync(
                            text,
                            result => {
                                console.log('Avatar finished speaking');
                                resolve(result);
                            },
                            error => {
                                console.error('Avatar speech error:', error);
                                reject(error);
                            }
                        );
                    });
                } else {
                    // Fallback to regular TTS if avatar not available
                    await speakWithTTS(text);
                }
            } catch (error) {
                console.error('Error speaking:', error);
            }
            
            isAvatarSpeaking = false;
            updateUI('listening');
            updateProcessingStatus('Ready');
            
            // Process next in queue
            setTimeout(() => processAvatarQueue(), 500);
        }
        
        async function speakWithTTS(text) {
            return new Promise((resolve) => {
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'es-AR';
                utterance.onend = resolve;
                speechSynthesis.speak(utterance);
            });
        }
        
        // ================================
        // AUDIO CAPTURE
        // ================================
        
        async function startAudioCapture() {
            try {

                if (audioContext && audioContext.state === 'running') {
                    console.log('Audio capture already running');
                    return;        
                }

                console.log('Starting audio capture...');
                
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                const constraints = {
                    audio: {
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                };
                
                mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
                console.log('Microphone access granted');
                
                const source = audioContext.createMediaStreamSource(mediaStream);
                audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                
                audioProcessor.onaudioprocess = async (event) => {
                    if (!realtimeWebSocket || realtimeWebSocket.readyState !== WebSocket.OPEN) return;
                    
                    const inputData = event.inputBuffer.getChannelData(0);
                    
                    // Convert to PCM16 and send to Realtime API
                    const pcm16 = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        const s = Math.max(-1, Math.min(1, inputData[i]));
                        pcm16[i] = s < 0 ? s * 32768 : s * 32767;
                    }
                    
                    const audioData = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
                    
                    realtimeWebSocket.send(JSON.stringify({
                        type: "input_audio_buffer.append",
                        audio: audioData
                    }));
                };
                
                source.connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);
                
                console.log('Audio capture started');
                updateAudioQuality('Good');
                
            } catch (error) {
                console.error('Audio capture failed:', error);
                throw error;
            }
        }
        
        // ================================
        // SESSION CONTROL
        // ================================
        
        async function toggleSession() {
            const button = document.getElementById('controlButton');
            if (button.disabled) return;
            button.disabled = true;
            try {
                if (!sessionActive) {
                    await startSession();
                } else {
                    await stopSession();
                }
            } finally {
                button.disabled = false;
            }
        }
        
        async function startSession() {
            try {
                console.log('Starting hybrid session...');
                
                document.getElementById('loadingSpinner').style.display = 'block';
                document.getElementById('placeholderText').textContent = 'Initializing...';
                
                // Initialize system if needed
                if (!realtimeConfig) {
                    const initialized = await initializeSystem();
                    if (!initialized) {
                        throw new Error('System initialization failed');
                    }
                }
                
                // Connect to OpenAI Realtime API
                await connectToRealtimeAPI();
                
                // Initialize Speech Avatar
                await initializeAvatar();
                
                // Update UI
                document.getElementById('controlButton').classList.add('active');
                document.getElementById('controlButtonIcon').textContent = 'STOP';
                document.getElementById('loadingSpinner').style.display = 'none';
                
                updateStatus('Session active - Speak now');
                
                // Initial greeting from avatar
                const greeting = "Hello! I'm your AI assistant with visual avatar. How can I help you today?";
                addMessage('assistant', greeting);
                queueAvatarResponse(greeting);
                
            } catch (error) {
                console.error('Failed to start session:', error);
                showError(`Failed to start: ${error.message}`);
                document.getElementById('loadingSpinner').style.display = 'none';
                document.getElementById('placeholderText').textContent = 'Avatar Offline';
            }
        }
        
        async function stopSession() {
            try {
                console.log('Stopping session...');
                
                sessionActive = false;
                
                // Close connections
                if (realtimeWebSocket) {
                    realtimeWebSocket.close();
                    realtimeWebSocket = null;
                }
                
                if (avatarSynthesizer) {
                    avatarSynthesizer.stopAvatarAsync();
                    avatarSynthesizer.close();
                    avatarSynthesizer = null;
                }
                
                if (peerConnection) {
                    peerConnection.close();
                    peerConnection = null;
                }
                
                // Stop audio
                if (audioProcessor) {
                    audioProcessor.disconnect();
                    audioProcessor = null;
                }
                
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => track.stop());
                    mediaStream = null;
                }
                
                if (audioContext) {
                    await audioContext.close();
                    audioContext = null;
                }
                
                // Clear timers
                if (sessionTimer) {
                    clearInterval(sessionTimer);
                    sessionTimer = null;
                }
                
                // Reset UI
                document.getElementById('controlButton').classList.remove('active');
                document.getElementById('controlButtonIcon').textContent = 'MIC';
                document.getElementById('avatarPlaceholder').style.display = 'flex';
                document.getElementById('placeholderText').textContent = 'Avatar Offline';
                document.getElementById('speechAvatarVideo').srcObject = null;
                
                updateStatus('Session ended - Ready to start');
                updateRealtimeStatus('Idle');
                updateAvatarStatus('Disconnected');
                updateVADStatus('Inactive');
                updateAudioQuality('--');
                updateProcessingStatus('Ready');
                
                avatarConnected = false;
                responseQueue = [];
                isAvatarSpeaking = false;
                
            } catch (error) {
                console.error('Error stopping session:', error);
            }
        }
        
        // ================================
        // UI UPDATES
        // ================================
        
        function updateUI(state) {
            const container = document.getElementById('avatarContainer');
            container.className = 'avatar-container';
            
            switch (state) {
                case 'listening':
                    container.classList.add('listening');
                    break;
                case 'thinking':
                    container.classList.add('thinking');
                    break;
                case 'speaking':
                    container.classList.add('speaking');
                    break;
            }
        }
        
        function updateStatus(message) {
            document.getElementById('status').textContent = message;
        }
        
        function updateRealtimeStatus(status) {
            document.getElementById('realtimeStatus').textContent = status;
        }
        
        function updateAvatarStatus(status) {
            document.getElementById('avatarStatus').textContent = status;
        }
        
        function updateVADStatus(status) {
            document.getElementById('vadStatus').textContent = status;
        }
        
        function updateAudioQuality(quality) {
            document.getElementById('audioQuality').textContent = quality;
        }
        
        function updateProcessingStatus(status) {
            document.getElementById('processingStatus').textContent = status;
        }
        
        function addMessage(role, text) {
            const conversation = document.getElementById('conversation');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            
            const roleName = role === 'user' ? 'You' : role === 'assistant' ? 'Assistant' : 'System';
            messageDiv.innerHTML = `<strong>${roleName}:</strong> ${text}`;
            
            conversation.appendChild(messageDiv);
            conversation.scrollTop = conversation.scrollHeight;
        }
        
        function clearConversation() {
            const conversation = document.getElementById('conversation');
            conversation.innerHTML = `
                <div class="message system">
                    <strong>System:</strong>
                    Conversation cleared. Ready for new interaction.
                </div>
            `;
        }
        
        function showError(message) {
            const errorDiv = document.createElement('div');
            errorDiv.className = 'error-message';
            errorDiv.textContent = message;
            document.querySelector('.container').appendChild(errorDiv);
            setTimeout(() => errorDiv.remove(), 5000);
        }
        
        function startSessionTimer() {
            sessionStartTime = Date.now();
            
            sessionTimer = setInterval(() => {
                const elapsed = Date.now() - sessionStartTime;
                const minutes = Math.floor(elapsed / 60000);
                const seconds = Math.floor((elapsed % 60000) / 1000);
                
                const timeString = `${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')}`;
                document.getElementById('sessionTime').textContent = timeString;
            }, 1000);
        }
        
        // Test function for avatar
        async function testAvatarSpeak() {
            if (!avatarConnected) {
                showError('Avatar not connected. Start a session first.');
                return;
            }
            
            const testMessage = "This is a test of the Azure Speech Avatar system with lip synchronization.";
            addMessage('system', `Testing avatar: "${testMessage}"`);
            queueAvatarResponse(testMessage);
        }
        
        // ================================
        // INITIALIZATION
        // ================================
        
        window.addEventListener('load', () => {
            console.log('Azure Hybrid System (Speech Avatar + OpenAI Realtime) Ready');
            updateStatus('System ready - Click to start');
        });
        
        // Cleanup on page unload
        window.addEventListener('beforeunload', async () => {
            if (sessionActive) {
                await stopSession();
            }
        });
    </script>
</body>
</html>